{
    "name": "BernoulliNB",
    "description": "Naive Bayes classifier for multivariate Bernoulli models.",
    "url": "http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html",
    "schema": {
        "alpha": {
            "description": "Additive (Laplace/Lidstone) smoothing parameter (0 for no smoothing).",
            "type": "float",
            "default": 1,
            "ui": {
                "style": "radio",
                "choices": [0.001, 0.01, 0.1, 1, 10, 100],
                "values": [0.001, 0.01, 0.1, 1, 10, 100]
            }
        },
        "binarize": {
            "description": "Threshold for binarizing (mapping to booleans) of sample features. If None, input is presumed to already consist of binary vectors.",
            "type": "float",
            "default": 0,
            "ui": {
                "style": "radio",
                "choices": [0, 0.25, 0.5, 0.75, 1],
                "values": [0, 0.25, 0.5, 0.75, 1]
            }
        },
        "fit_prior": {
            "description": "Whether to learn class prior probabilities or not. If false, a uniform prior will be used.",
            "type": "bool",
            "default": "true",
            "ui": {
                "style": "radio",
                "choices": ["true", "false"],
                "values": ["True", "False"]
            }
        }
    },
    "category": "ML"
} {
    "name": "GaussianNB",
    "description": "Gaussian Naive Bayes",
    "url": "http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html",
    "schema": {},
    "category": "ML"
} {
    "name": "MultinomialNB",
    "description": "Naive Bayes classifier for multinomial models.",
    "url": "http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html",
    "schema": {
        "alpha": {
            "description": "Additive (Laplace/Lidstone) smoothing parameter (0 for no smoothing).",
            "type": "float",
            "default": 1.0,
            "ui": {
                "style": "radio",
                "choices": [0.001, 0.01, 0.1, 1.0, 10, 100],
                "values": [0.001, 0.01, 0.1, 1.0, 10, 100]
            }
        },
        "fit_prior": {
            "description": "Whether to learn class prior probabilities or not. If false, a uniform prior will be used.",
            "type": "bool",
            "default": "true",
            "ui": {
                "style": "radio",
                "choices": ["true", "false"],
                "values": ["True", "False"]
            }
        }
    },
    "category": "ML"
} {
    "name": "DecisionTreeClassifier",
    "description": "Classifier that assigns a class to a sample based on a chained series of yes/no queries about the sample's features.",
    "url": "http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html",
    "schema": {
        "criterion": {
            "description": "The function to measure the quality of a split. Supported criteria are “gini” for the Gini impurity and “entropy” for the information gain.",
            "type": "enum",
            "default": "gini",
            "ui": {
                "style": "radio",
                "choices": ["gini", "entropy"],
                "values": ["Gini impurity", "Information gain"]
            }
        },
        "max_depth": {
            "description": "The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.",
            "type": "int",
            "default": 2,
            "ui": {
                "style": "radio",
                "choices": [2, 4, 6, 8],
                "values": [2, 4, 6, 8]
            }
        },
        "min_samples_split": {
            "description": "The minimum number of samples required to split an internal node.",
            "type": "int",
            "default": 5,
            "ui": {
                "style": "radio",
                "choices": [5, 10, 20],
                "values": [5, 10, 20]
            }
        },
        "min_samples_leaf": {
            "description": "The minimum number of samples required to be at a leaf node.",
            "type": "int",
            "default": 5,
            "ui": {
                "style": "radio",
                "choices": [5, 10, 20],
                "values": [5, 10, 20]
            }
        }
    },
    "category": "ML"
} {
    "name": "ExtraTreesClassifier",
    "description": "Extremely Randomized Trees",
    "url": "http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html",
    "schema": {
        "n_estimators": {
            "description": "The number of trees in the forest.",
            "type": "int",
            "default": 100,
            "ui": {
                "style": "radio",
                "choices": [100, 250],
                "values": [100, 250]
            }
        },
        "criterion": {
            "description": "The function to measure the quality of a split. Supported criteria are “gini” for the Gini impurity and “entropy” for the information gain.",
            "type": "enum",
            "values": ["gini", "entropy"],
            "default": "gini",
            "ui": {
                "style": "radio",
                "choices": ["gini", "entropy"],
                "values": ["Gini impurity", "Information gain"]
            }
        },
        "max_features": {
            "description": "The number of features to consider when looking for the best split.",
            "type": "string",
            "default": "sqrt",
            "ui": {
                "style": "radio",
                "choices": ["sqrt", "log2", "None"],
                "values": ["Square root", "Log2", "None"]
            }
        },
        "min_samples_split": {
            "description": "The minimum number of samples required to split an internal node.",
            "type": "int",
            "default": 5,
            "ui": {
                "style": "radio",
                "choices": [5, 10, 20],
                "values": [5, 10, 20]
            }
        },
        "min_samples_leaf": {
            "description": "The minimum number of samples required to be at a leaf node.",
            "type": "int",
            "default": 5,
            "ui": {
                "style": "radio",
                "choices": [5, 10, 20],
                "values": [5, 10, 20]
            }
        },
        "bootstrap": {
            "description": "Whether bootstrap samples are used when building trees.",
            "type": "bool",
            "default": "false",
            "ui": {
                "style": "radio",
                "choices": ["true", "false"],
                "values": ["True", "False"]
            }
        }
    },
    "category": "ML"
} {
    "name": "GradientBoostingClassifier",
    "description": "An ensemble of decision trees that are iteratively trained on the dataset to minimize classification accuracy.",
    "url": "http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html",
    "schema": {
        "n_estimators": {
            "description": "The number of boosting stages to perform. Gradient boosting is fairly robust to over-fitting so a large number usually results in better performance.",
            "type": "int",
            "default": 100,
            "ui": {
                "style": "radio",
                "choices": [100, 250],
                "values": [100, 250]
            }
        },
        "learning_rate": {
            "description": "Learning rate shrinks the contribution of each tree by learning_rate. There is a trade-off between learning_rate and n_estimators.",
            "type": "float",
            "default": 0.1,
            "ui": {
                "style": "radio",
                "choices": [0.01, 0.1, 1],
                "values": [0.01, 0.1, 1]
            }
        },
        "max_depth": {
            "description": "Maximum depth of the individual regression estimators. The maximum depth limits the number of nodes in the tree. Tune this parameter for best performance; the best value depends on the interaction of the input variables.",
            "type": "int",
            "default": 1,
            "ui": {
                "style": "radio",
                "choices": [1, 5, 10],
                "values": [1, 5, 10]
            }
        },
        "min_samples_split": {
            "description": "The minimum number of samples required to split an internal node.",
            "type": "int",
            "default": 5,
            "ui": {
                "style": "radio",
                "choices": [5, 10, 20],
                "values": [5, 10, 20]
            }
        },
        "min_samples_leaf": {
            "description": "The minimum number of samples required to be at a leaf node.",
            "type": "int",
            "default": 5,
            "ui": {
                "style": "radio",
                "choices": [5, 10, 20],
                "values": [5, 10, 20]
            }
        },
        "subsample": {
            "description": "The fraction of samples to be used for fitting the individual base learners. If smaller than 1.0 this results in Stochastic Gradient Boosting. subsample interacts with the parameter n_estimators. Choosing subsample \u003c 1.0 leads to a reduction of variance and an increase in bias.",
            "type": "float",
            "default": 1,
            "ui": {
                "style": "radio",
                "choices": [0.5, 1],
                "values": [0.5, 1]
            }
        },
        "max_features": {
            "description": "The number of features to consider when looking for the best split.",
            "type": "string",
            "default": "sqrt",
            "ui": {
                "style": "radio",
                "choices": ["sqrt", "log2"],
                "values": ["Square root", "Log2"]
            }
        }
    },
    "category": "ML"
} {
    "name": "KNeighborsClassifier",
    "description": "Nearest-neighbor classifier that classifies new data points based on the most common class among the k nearest data points.",
    "url": "http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html",
    "schema": {
        "n_neighbors": {
            "description": "Number of neighbors to use by default for k_neighbors queries.",
            "type": "int",
            "default": 5,
            "ui": {
                "style": "radio",
                "choices": [1, 3, 5, 7, 9, 11],
                "values": [1, 3, 5, 7, 9, 11]
            }
        },
        "weights": {
            "description": "Weight function used in prediction.",
            "type": "string",
            "default": "uniform",
            "ui": {
                "style": "radio",
                "choices": ["uniform", "distance"],
                "values": ["Uniform", "Distance"]
            }
        },
        "p": {
            "description": "Power parameter for the Minkowski metric.",
            "type": "int",
            "default": 2,
            "ui": {
                "style": "radio",
                "choices": [1, 2],
                "values": [1, 2]
            }
        }
    },
    "category": "ML"
} {
    "name": "LinearSVC",
    "description": "Linear Support Vector Classification.",
    "url": "http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html",
    "schema": {
        "penalty": {
            "description": "Specifies the norm used in the penalization. The ‘l2’ penalty is the standard used in SVC. The ‘l1’ leads to coef_ vectors that are sparse.",
            "type": "string",
            "default": "l2",
            "ui": {
                "style": "radio",
                "choices": ["l1", "l2"],
                "values": ["L1", "L2"]
            }
        },
        "loss": {
            "description": "Specifies the loss function. ‘hinge’ is the standard SVM loss (used e.g. by the SVC class) while ‘squared_hinge’ is the square of the hinge loss.",
            "type": "string",
            "default": "squared_hinge",
            "ui": {
                "style": "radio",
                "choices": ["hinge", "squared_hinge"],
                "values": ["Hinge", "Squared hinge"]
            }
        },
        "dual": {
            "description": "Select the algorithm to either solve the dual or primal optimization problem. Prefer dual=False when n_samples \u003e n_features.",
            "type": "bool",
            "default": "true",
            "ui": {
                "style": "radio",
                "choices": ["true", "false"],
                "values": ["True", "False"]
            }
        },
        "tol": {
            "description": "Tolerance for stopping criteria.",
            "type": "float",
            "default": 0.0001,
            "ui": {
                "style": "radio",
                "choices": [1e-05, 0.0001, 0.001, 0.01, 0.1],
                "values": [1e-05, 0.0001, 0.001, 0.01, 0.1]
            }
        },
        "C": {
            "description": "Penalty parameter C of the error term.",
            "type": "float",
            "default": 1,
            "ui": {
                "style": "radio",
                "choices": [0.0001, 0.001, 0.01, 0.1, 0.5, 1, 10, 25],
                "values": [0.0001, 0.001, 0.01, 0.1, 0.5, 1, 10, 25]
            }
        }
    },
    "category": "ML"
} {
    "name": "LogisticRegression",
    "description": "Basic logistic regression that makes predictions about the outcome based on a linear combination of the features.",
    "url": "http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html",
    "schema": {
        "penalty": {
            "description": "Used to specify the norm used in the penalization. The ‘newton-cg’, ‘sag’ and ‘lbfgs’ solvers support only l2 penalties.",
            "type": "string",
            "default": "l2",
            "ui": {
                "style": "radio",
                "choices": ["l1", "l2"],
                "values": ["L1", "L2"]
            }
        },
        "C": {
            "description": "Inverse of regularization strength; must be a positive float. Like in support vector machines, smaller values specify stronger regularization.",
            "type": "float",
            "default": 1.0,
            "ui": {
                "style": "radio",
                "choices": [0.0001, 0.001, 0.01, 0.1, 0.5, 1, 10, 25],
                "values": [0.0001, 0.001, 0.01, 0.1, 0.5, 1, 10, 25]
            }
        },
        "dual": {
            "description": "Select the algorithm to either solve the dual or primal optimization problem. Prefer dual=False when n_samples \u003e n_features.",
            "type": "bool",
            "default": "false",
            "ui": {
                "style": "radio",
                "choices": ["true", "false"],
                "values": ["True", "False"]
            }
        }
    },
    "category": "ML"
} {
    "name": "RandomForestClassifier",
    "description": "An ensemble of decision trees that are trained on random sub-samples of the dataset.",
    "url": "http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html",
    "schema": {
        "n_estimators": {
            "description": "The number of trees in the forest.",
            "type": "int",
            "default": 100,
            "ui": {
                "style": "radio",
                "choices": [100, 250],
                "values": [100, 250]
            }
        },
        "criterion": {
            "description": "The function to measure the quality of a split. Supported criteria are “gini” for the Gini impurity and “entropy” for the information gain. Note: this parameter is tree-specific.",
            "type": "string",
            "default": "gini",
            "ui": {
                "style": "radio",
                "choices": ["gini", "entropy"],
                "values": ["Gini impurity", "Information gain"]
            }
        },
        "max_features": {
            "description": "The number of features to consider when looking for the best split.",
            "type": "string",
            "default": "sqrt",
            "ui": {
                "style": "radio",
                "choices": ["sqrt", "log2"],
                "values": ["Square root", "Log2"]
            }
        },
        "min_samples_split": {
            "description": "The minimum number of samples required to split an internal node.",
            "type": "int",
            "default": 5,
            "ui": {
                "style": "radio",
                "choices": [5, 10, 20],
                "values": [5, 10, 20]
            }
        },
        "min_samples_leaf": {
            "description": "The minimum number of samples required to be at a leaf node.",
            "type": "int",
            "default": 5,
            "ui": {
                "style": "radio",
                "choices": [5, 10, 20],
                "values": [5, 10, 20]
            }
        },
        "bootstrap": {
            "description": "Whether bootstrap samples are used when building trees.",
            "type": "bool",
            "default": "true",
            "ui": {
                "style": "radio",
                "choices": ["true", "false"],
                "values": ["True", "False"]
            }
        }
    },
    "category": "ML"
} {
    "name": "SVC",
    "description": "Kernel-based classifier that maps the data into a high-dimesional space then constructs a hyperplane that maximally separates the classes in that high-dimesional space.",
    "url": "http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html",
    "schema": {
        "kernel": {
            "description": "Specifies the kernel type to be used in the algorithm",
            "type": "string",
            "default": "rbf",
            "ui": {
                "style": "radio",
                "choices": ["poly", "rbf"],
                "values": ["Polynomial", "Radial basis function"]
            }
        },
        "tol": {
            "description": "Tolerance for stopping criteria.",
            "type": "float",
            "default": 0.0001,
            "ui": {
                "style": "radio",
                "choices": [1e-05, 0.0001, 0.001, 0.01, 0.1],
                "values": [1e-05, 0.0001, 0.001, 0.01, 0.1]
            }
        },
        "C": {
            "description": "Penalty parameter C of the error term.",
            "type": "float",
            "default": 1,
            "ui": {
                "style": "radio",
                "choices": [0.0001, 0.001, 0.01, 0.1, 0.5, 1, 10, 25],
                "values": [0.0001, 0.001, 0.01, 0.1, 0.5, 1, 10, 25]
            }
        }
    },
    "category": "ML"
}
