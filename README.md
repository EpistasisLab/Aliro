# Penn AI
Engine for reading in modeling results, updating knowledge base, and making recommendations that instantiate new runs. 
## workflow
 - The Penn AI agent looks for new requests for recommendations and new experimental results every 5 seconds. 
 - when a new experiment is found, it is used to update the recommender. 
 - when a new request is received, the AI retreives a recommendation from the recommender and pushes it to the user. 
### Recommender
```python
pennai = Recommender(method='ml_p',ml_type='classifier')
# data: a dataframe of results from database
pennai.update(results_data)
``` 
 - given a new modeling task, the AI recommends an ML method with parameter values (P)
```python
# dataset_metafeatures: an optional set of metafeatures of the dataset to assist in recommendations
ml,p = pennai.recommend(dataset_metafeatures=None)
```
 - the ML+P recommendation is run on the dataset using the AI system

```python
ai.send_rec()
```
 - the results are used to update the recommender
```python
pennai.update(new_results_data)
```
## overall tasks
 - [x] build dataframe `results_data` from MongoDB results. 
  - Dataframe cols:
  - 'classifier' or 'regressor': e.g. "RandomForestClassifier"
  - ML parameter data:
  - metrics: 'accuracy','bal_accuracy', 'AUC': score of run
  - dataset: dataset name e.g. "iris"
  - metafeatures: list from `dataset_describe.py`
 - [x] make method to post job submissions 
 - [ ] recommendation shows up in launch page

## recommender tasks
- [x] filter recommendations for what has already been run
- [x] direct acess to MongDB results for checking what has been run

recommendations using:
 - [x] ml + p 
 - [ ] ml + p + mf
 - [ ] ml + p + mf, per model basis
 - [ ] incorporating expert knowledge rules
 - [ ] analyze which metafeatures are important
 - [x ] make method to submit jobs (`submit(dataset,ml,p)`)

