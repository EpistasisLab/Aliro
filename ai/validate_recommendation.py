import numpy as np
import pdb

def find_nearest(array,value):
    idx = (np.abs(array-value)).argmin()
    return array[idx]

ui_options = [
{"_id":"57bf24e1548cd20008bc71df","name":"GP-simplified","schema":{"primitives":{"description":"GP primitives (operators/operands)","type":"enum-checkbox","values":["+","-","*","/","neg","ephemeral"],"default":["+","-","*","/","neg","ephemeral"]},"population-size":{"description":"number of individuals","type":"enum-radio","values":[10,100,1000],"default":100},"generations":{"description":"number of generations, n>0 [default = 1000]","type":"enum-radio","values":[10,100,1000],"default":100},"crossover-probability":{"description":"crossover probability, 0.0<=f<=1.0","type":"enum-radio","values":[0.1,0.5,0.9],"default":0.9},"mutation-probability":{"description":"0.0<=f<=1.0","type":"enum-radio","values":[0.001,0.01,0.1],"default":0.1},"seletion-pressure":{"description":"selection pressure (tournament size), n>=1","type":"enum-radio","values":[1,2,3],"default":3},"elitism-size":{"description":"elitism size, 0<=n<=1","type":"enum-radio","values":[0,0.5,1],"default":1},"maximum-size ":{"description":"maximum program size","type":"enum-radio","values":[5,10,20],"default":20},"minimum-size":{"description":"minimum program size","type":"enum-radio","values":[1,2,3],"default":1}},"category":"ML"},
{"_id":"57c5a83f4b09d40023ee4f6e","name":"MDR.js","schema":{"default-label":{"description":"Default Label","type":"enum-radio","values":[0,1],"default":0},"tie-break":{"description":"Tie Break","type":"enum-radio","values":[0,1],"default":1}},"category":"ML"},
{"_id":"5813bdaec1321420f8bbcc7f","name":"BernoulliNB","schema":{"alpha":{"description":"Additive (Laplace/Lidstone) smoothing parameter (0 for no smoothing).","type":"float","default":1,"ui":{"style":"radio","choices":[0.001,0.01,0.1,1,10,100]}},"binarize":{"description":"Threshold for binarizing (mapping to booleans) of sample features. If None, input is presumed to already consist of binary vectors.","type":"float","default":0,"ui":{"style":"radio","choices":[0,0.25,0.5,0.75,1]}},"fit_prior":{"description":"Whether to learn class prior probabilities or not. If False, a uniform prior will be used.","type":"bool","default":True,"ui":{"style":"radio","choices":[True,False]}}},"category":"ML"},
{"_id":"5813bdb8c1321420f8bbcc80","name":"GaussianNB","schema":{},"category":"ML"},
{"_id":"5813bdc4c1321420f8bbcc81","name":"MultinomialNB","schema":{"alpha":{"description":"Additive (Laplace/Lidstone) smoothing parameter (0 for no smoothing).","type":"float","default":1,"ui":{"style":"radio","choices":[0.001,0.01,0.1,1,10,100]}},"fit_prior":{"description":"Whether to learn class prior probabilities or not. If False, a uniform prior will be used.","type":"bool","default":True,"ui":{"style":"radio","choices":[True,False]}}},"category":"ML"},
{"_id":"5817660338215335404347c7","name":"DecisionTreeClassifier","schema":{"criterion":{"description":"The function to measure the quality of a split. Supported criteria are “gini” for the Gini impurity and “entropy” for the information gain.","type":"enum","values":["gini","entropy"],"default":"gini","ui":{"style":"radio","choices":["gini","entropy"]}},"max_depth":{"description":"The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.","type":"int","default":"None","ui":{"style":"radio","choices":[1,3,5,7,9,"None"]}},"min_samples_split":{"description":"The minimum number of samples required to split an internal node.","type":"int","default":2,"ui":{"style":"radio","choices":[2,5,10,15,20]}},"min_samples_leaf":{"description":"The minimum number of samples required to be at a leaf node.","type":"int","default":1,"ui":{"style":"radio","choices":[1,5,10,15,20]}}},"category":"ML"},
{"_id":"581791063821533540434826","name":"ExtraTreesClassifier","schema":{"n_estimators":{"description":"The number of trees in the forest.","type":"int","default":100,"ui":{"style":"radio","choices":[10,100,500,1000]}},"criterion":{"description":"The function to measure the quality of a split. Supported criteria are “gini” for the Gini impurity and “entropy” for the information gain.","type":"enum","values":["gini","entropy"],"default":"gini","ui":{"style":"radio","choices":["gini","entropy"]}},"max_features":{"description":"The number of features to consider when looking for the best split.","type":"string","default":"sqrt","ui":{"style":"radio","choices":["sqrt","log2","None"]}},"min_samples_split":{"description":"The minimum number of samples required to split an internal node.","type":"int","default":2,"ui":{"style":"radio","choices":[2,5,10,15,20]}},"min_samples_leaf":{"description":"The minimum number of samples required to be at a leaf node.","type":"int","default":1,"ui":{"style":"radio","choices":[1,5,10,15,20]}},"bootstrap":{"description":"Whether bootstrap samples are used when building trees.","type":"bool","default":False,"ui":{"style":"radio","choices":[True,False]}}},"category":"ML"},
{"_id":"581796a43821533540434890","name":"GradientBoostingClassifier","schema":{"n_estimators":{"description":"The number of boosting stages to perform. Gradient boosting is fairly robust to over-fitting so a large number usually results in better performance.","type":"int","default":100,"ui":{"style":"radio","choices":[10,100,500,1000]}},"learning_rate":{"description":"Learning rate shrinks the contribution of each tree by learning_rate. There is a trade-off between learning_rate and n_estimators.","type":"float","default":0.1,"ui":{"style":"radio","choices":[0.001,0.01,0.1,0.5,1]}},"max_depth":{"description":"Maximum depth of the individual regression estimators. The maximum depth limits the number of nodes in the tree. Tune this parameter for best performance; the best value depends on the interaction of the input variables.","type":"int","default":3,"ui":{"style":"radio","choices":[1,3,5,7,9,"None"]}},"min_samples_split":{"description":"The minimum number of samples required to split an internal node.","type":"int","default":2,"ui":{"style":"radio","choices":[2,5,10,15,20]}},"min_samples_leaf":{"description":"The minimum number of samples required to be at a leaf node.","type":"int","default":1,"ui":{"style":"radio","choices":[1,5,10,15,20]}},"subsample":{"description":"The fraction of samples to be used for fitting the individual base learners. If smaller than 1.0 this results in Stochastic Gradient Boosting. subsample interacts with the parameter n_estimators. Choosing subsample < 1.0 leads to a reduction of variance and an increase in bias.","type":"float","default":1,"ui":{"style":"radio","choices":[0.05,0.25,0.5,0.75,1]}},"max_features":{"description":"The number of features to consider when looking for the best split.","type":"string","default":"None","ui":{"style":"radio","choices":["sqrt","log2","None"]}}},"category":"ML"},
{"_id":"5817a21138215335404348cd","name":"KNeighborsClassifier","schema":{"n_neighbors":{"description":"Number of neighbors to use by default for k_neighbors queries.","type":"int","default":5,"ui":{"style":"radio","choices":[1,3,5,7,9,11]}},"weights":{"description":"Weight function used in prediction.","type":"string","default":"uniform","ui":{"style":"radio","choices":["uniform","distance"]}},"p":{"description":"Power parameter for the Minkowski metric.","type":"int","default":2,"ui":{"style":"radio","choices":[1,2]}}},"category":"ML"},
{"_id":"5817a73538215335404348ee","name":"LinearSVC","schema":{"penalty":{"description":"Specifies the norm used in the penalization. The ‘l2’ penalty is the standard used in SVC. The ‘l1’ leads to coef_ vectors that are sparse.","type":"string","default":"l2","ui":{"style":"radio","choices":["l1","l2"]}},"loss":{"description":"Specifies the loss function. ‘hinge’ is the standard SVM loss (used e.g. by the SVC class) while ‘squared_hinge’ is the square of the hinge loss.","type":"string","default":"squared_hinge","ui":{"style":"radio","choices":["hinge","squared_hinge"]}},"dual":{"description":"Select the algorithm to either solve the dual or primal optimization problem. Prefer dual=False when n_samples > n_features.","type":"bool","default":True,"ui":{"style":"radio","choices":[True,False]}},"tol":{"description":"Tolerance for stopping criteria.","type":"float","default":0.0001,"ui":{"style":"radio","choices":[0.00001,0.0001,0.001,0.01,0.1]}},"C":{"description":"Penalty parameter C of the error term.","type":"float","default":1,"ui":{"style":"radio","choices":[0.0001,0.001,0.01,0.1,0.5,1,10,25]}}},"category":"ML"},
{"_id":"5817ad7f3821533540434948","name":"LogisticRegression","schema":{"penalty":{"description":"Used to specify the norm used in the penalization. The ‘newton-cg’, ‘sag’ and ‘lbfgs’ solvers support only l2 penalties.","type":"string","default":"l2","ui":{"style":"radio","choices":["l1","l2"]}},"C":{"description":"Inverse of regularization strength; must be a positive float. Like in support vector machines, smaller values specify stronger regularization.","type":"float","default":1,"ui":{"style":"radio","choices":[0.0001,0.001,0.01,0.1,0.5,1,10,25]}},"dual":{"description":"Select the algorithm to either solve the dual or primal optimization problem. Prefer dual=False when n_samples > n_features.","type":"bool","default":False,"ui":{"style":"radio","choices":[True,False]}}},"category":"ML"},
{"_id":"5817af52382153354043496e","name":"RandomForestClassifier","schema":{"n_estimators":{"description":"The number of trees in the forest.","type":"int","default":100,"ui":{"style":"radio","choices":[10,100,500,1000]}},"criterion":{"description":"The function to measure the quality of a split. Supported criteria are “gini” for the Gini impurity and “entropy” for the information gain. Note: this parameter is tree-specific.","type":"string","default":"gini","ui":{"style":"radio","choices":["gini","entropy"]}},"max_features":{"description":"The number of features to consider when looking for the best split.","type":"string","default":"sqrt","ui":{"style":"radio","choices":["sqrt","log2","None"]}},"min_samples_split":{"description":"The minimum number of samples required to split an internal node.","type":"int","default":2,"ui":{"style":"radio","choices":[2,5,10,15,20]}},"min_samples_leaf":{"description":"The minimum number of samples required to be at a leaf node.","type":"int","default":1,"ui":{"style":"radio","choices":[1,5,10,15,20]}},"bootstrap":{"description":"Whether bootstrap samples are used when building trees.","type":"bool","default":True,"ui":{"style":"radio","choices":[True,False]}}},"category":"ML"},
{"_id":"5853571ca52513003444131b","name":"DecisionTreeRegressor","schema":{"max_depth":{"description":"The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.","type":"int","default":"None","ui":{"style":"radio","choices":[1,3,5,7,9,"None"]}},"min_samples_split":{"description":"The minimum number of samples required to split an internal node.","type":"int","default":2,"ui":{"style":"radio","choices":[2,5,10,15,20]}},"min_samples_leaf":{"description":"The minimum number of samples required to be at a leaf node.","type":"int","default":1,"ui":{"style":"radio","choices":[1,5,10,15,20]}}},"category":"ML"},
{"_id":"58535850a52513003444131e","name":"ElasticNetCV","schema":{"l1_ratio":{"description":"Float between 0 and 1 passed to ElasticNet (scaling between l1 and l2 penalties).","type":"float","default":0.5,"ui":{"style":"radio","choices":[0,0.25,0.5,0.75,1]}},"tol":{"description":"The tolerance for the optimization: if the updates are smaller than tol, the optimization code checks the dual gap for optimality and continues until it is smaller than tol.","type":"float","default":0.0001,"ui":{"style":"radio","choices":[0.00001,0.0001,0.001,0.01,0.1]}}},"category":"ML"},
{"_id":"585358eba52513003444131f","name":"ExtraTreesRegressor","schema":{"n_estimators":{"description":"The number of trees in the forest.","type":"int","default":100,"ui":{"style":"radio","choices":[10,100,500,1000]}},"max_features":{"description":"The number of features to consider when looking for the best split.","type":"string","default":"sqrt","ui":{"style":"radio","choices":["sqrt","log2","None"]}},"min_samples_split":{"description":"The minimum number of samples required to split an internal node.","type":"int","default":2,"ui":{"style":"radio","choices":[2,5,10,15,20]}},"min_samples_leaf":{"description":"The minimum number of samples required to be at a leaf node.","type":"int","default":1,"ui":{"style":"radio","choices":[1,5,10,15,20]}},"bootstrap":{"description":"Whether bootstrap samples are used when building trees.","type":"bool","default":False,"ui":{"style":"radio","choices":[True,False]}}},"category":"ML"},
{"_id":"58535984a525130034441320","name":"GradientBoostingRegressor","schema":{"n_estimators":{"description":"The number of boosting stages to perform. Gradient boosting is fairly robust to over-fitting so a large number usually results in better performance.","type":"int","default":100,"ui":{"style":"radio","choices":[10,100,500,1000]}},"learning_rate":{"description":"Learning rate shrinks the contribution of each tree by learning_rate. There is a trade-off between learning_rate and n_estimators.","type":"float","default":0.1,"ui":{"style":"radio","choices":[0.001,0.01,0.1,0.5,1]}},"max_depth":{"description":"Maximum depth of the individual regression estimators. The maximum depth limits the number of nodes in the tree. Tune this parameter for best performance; the best value depends on the interaction of the input variables.","type":"int","default":3,"ui":{"style":"radio","choices":[1,3,5,7,9,"None"]}},"min_samples_split":{"description":"The minimum number of samples required to split an internal node.","type":"int","default":2,"ui":{"style":"radio","choices":[2,5,10,15,20]}},"min_samples_leaf":{"description":"The minimum number of samples required to be at a leaf node.","type":"int","default":1,"ui":{"style":"radio","choices":[1,5,10,15,20]}},"subsample":{"description":"The fraction of samples to be used for fitting the individual base learners. If smaller than 1.0 this results in Stochastic Gradient Boosting. subsample interacts with the parameter n_estimators. Choosing subsample < 1.0 leads to a reduction of variance and an increase in bias.","type":"float","default":1,"ui":{"style":"radio","choices":[0.05,0.25,0.5,0.75,1]}},"max_features":{"description":"The number of features to consider when looking for the best split.","type":"string","default":"None","ui":{"style":"radio","choices":["sqrt","log2","None"]}}},"category":"ML"},
{"_id":"58535a05a525130034441321","name":"KNeighborsRegressor","schema":{"n_neighbors":{"description":"Number of neighbors to use by default for k_neighbors queries.","type":"int","default":5,"ui":{"style":"radio","choices":[1,3,5,7,9,11]}},"weights":{"description":"Weight function used in prediction.","type":"string","default":"uniform","ui":{"style":"radio","choices":["uniform","distance"]}},"p":{"description":"Power parameter for the Minkowski metric.","type":"int","default":2,"ui":{"style":"radio","choices":[1,2]}}},"category":"ML"},
{"_id":"58535a5aa525130034441322","name":"LassoLarsCV","schema":{},"category":"ML"},
{"_id":"58535b31a525130034441326","name":"LinearSVR","schema":{"loss":{"description":"Specifies the loss function. ‘l1’ is the epsilon-insensitive loss (standard SVR) while ‘l2’ is the squared epsilon-insensitive loss.","type":"string","default":"epsilon_insensitive","ui":{"style":"radio","choices":["epsilon_insensitive","squared_epsilon_insensitive"]}},"dual":{"description":"Select the algorithm to either solve the dual or primal optimization problem. Prefer dual=False when n_samples > n_features.","type":"bool","default":True,"ui":{"style":"radio","choices":[True,False]}},"tol":{"description":"Tolerance for stopping criteria.","type":"float","default":0.0001,"ui":{"style":"radio","choices":[0.00001,0.0001,0.001,0.01,0.1]}},"C":{"description":"Penalty parameter C of the error term. The penalty is a squared l2 penalty. The bigger this parameter, the less regularization is used.","type":"float","default":1,"ui":{"style":"radio","choices":[0.0001,0.001,0.01,0.1,0.5,1,10,25]}},"epsilon":{"description":"Epsilon parameter in the epsilon-insensitive loss function. Note that the value of this parameter depends on the scale of the target variable y. If unsure, set epsilon=0.","type":"float","default":0,"ui":{"style":"radio","choices":[0,0.0001,0.001,0.01,0.1,1]}}},"category":"ML"},
{"_id":"58535b8fa525130034441327","name":"Meta-GA","schema":{"algorithms":{"description":"ML algorithms","type":"enum","values":["LogisticRegression","RandomForestClassifier","DecisionTreeClassifier","ExtraTreesClassifier","KNeighborsClassifier","GradientBoostingClassifier","LinearSVC","GaussianNB","MultinomialNB","deap-GP-SymbReg"],"default":"LogisticRegression","ui":{"style":"ratio","choices":["LogisticRegression","RandomForestClassifier","DecisionTreeClassifier","ExtraTreesClassifier","KNeighborsClassifier","GradientBoostingClassifier","LinearSVC","GaussianNB","MultinomialNB","deap-GP-SymbReg"]}},"meta_pop":{"description":"Population Size","type":"int","default":100,"ui":{"style":"radio","choices":[20,50,100,200,500,1000]}},"meta_gen":{"description":"Number of Generations","type":"int","default":30,"ui":{"style":"radio","choices":[5,10,30,100,200,500]}},"meta_cross_rt":{"description":"Crossover Rate","type":"float","default":0.5,"ui":{"style":"radio","choices":[0.01,0.1,0.3,0.5,0.7,1]}},"meta_mut_rt":{"description":"Mutation Rate","type":"float","default":0.3,"ui":{"style":"radio","choices":[0.01,0.1,0.3,0.5,0.7,1]}},"meta_tourn_size":{"description":"Tournament Size","type":"int","default":5,"ui":{"style":"radio","choices":[3,5,10,20]}}},"category":"AI"},
{"_id":"58535c7ea525130034441328","name":"RandomForestRegressor","schema":{"n_estimators":{"description":"The number of trees in the forest.","type":"int","default":100,"ui":{"style":"radio","choices":[10,100,500,1000]}},"max_features":{"description":"The number of features to consider when looking for the best split.","type":"string","default":"sqrt","ui":{"style":"radio","choices":["sqrt","log2","None"]}},"min_samples_split":{"description":"The minimum number of samples required to split an internal node.","type":"int","default":2,"ui":{"style":"radio","choices":[2,5,10,15,20]}},"min_samples_leaf":{"description":"The minimum number of samples required to be at a leaf node.","type":"int","default":1,"ui":{"style":"radio","choices":[1,5,10,15,20]}},"bootstrap":{"description":"Whether bootstrap samples are used when building trees.","type":"bool","default":False,"ui":{"style":"radio","choices":[True,False]}}},"category":"ML"},
{"_id":"58535d09a525130034441329","name":"RidgeCV","schema":{},"category":"ML"},
{"_id":"58535d64a52513003444132a","name":"deap-GP-SymbReg","schema":{"population_size":{"description":"number of individuals (default = 100)","type":"int","default":100,"ui":{"style":"radio","choices":[20,50,100,200,500]}},"generations":{"description":"number of generations, n>0 [default = 30]","type":"int","default":30,"ui":{"style":"radio","choices":[5,10,30,100,200]}},"crossover_rate":{"description":"crossover rate, 0.0<=f<=1.0, [default = 0.1]","type":"float","default":0.5,"ui":{"style":"radio","choices":[0.01,0.1,0.3,0.5,0.7,1]}},"mutation_rate":{"description":" mutation rate, 0.0<=f<=1.0, [default = 0.05]","type":"float","default":0.3,"ui":{"style":"radio","choices":[0.01,0.1,0.3,0.5,0.7,1]}},"tourn_size":{"description":"selection pressure (tournament size), n>=1 [default = 3]","type":"int","default":3,"ui":{"style":"radio","choices":[3,5,10,20]}}},"category":"ML"}
]

def validate_recs(ml,p):
    """Checks rec against possible settings for user. Removes parameters not
    available to the user and if parameter value is not available, it shifts
    the recommended value to the closest option."""
    match_ml = [ui_options[i] for i,op in enumerate(ui_options)
                if ml==op['_id']]
    pdb.set_trace()
    p_d = eval(p)
    p_new = eval(p)
    match_p_d = match_ml['schema']
    # loop through p_d and make sure keys match. then check values

    for k,v in p_d.items():
        if k in match_p_d.keys():
            if v not in match_p_d[k]['values']:
                if type(v) is str:
                    del p_new[k]
                else:
                    # set to closest value
                    p_new[k] = find_nearest(match_p_d[k]['values'], v)
        else:
            del p_new[k]

    return ml,str(p)
